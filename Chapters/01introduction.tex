% Chapter Template

\chapter{Introduction} % Main chapter title

\label{Chapter1} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{Chapter 1. \emph{Introduction}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title

These days, information is very important. Not only information itself, but the \emph{access} to it is central. Most information is found there, where everyone contributes to - the world wide web. Here we have many contributors (authors, engineers, teachers, ...) on many different sites and this means: fragmented information. Even if a certain piece of information is there we can't \emph{access} it, because we don't know where to find it. Thats unsatisfying and thats why search engines \footnote{\url{http://en.wikipedia.org/wiki/Web_search_engine}} are so important to us.
\newline
Unfortunately, today, only a few big companies are able to deal with both the engineering challenge and the resource requirements needed to operate a good search engine. They collect all the information and give us \emph{access} to it. What's even more frustrating is that we don't really know \emph{how} they do it. In order to better understand, research is ongoing in the diverse domains of search engines:
\newline
A lot of work has been done in page importance algorithms (selection policies) or what pages need to be crawled how often (re-visit policies)\cite{page_importance1}\cite{page_importance2}. At the end, assuming that we selected the optimal set of pages, we need to be able to download them in parallel and efficiently (parallelisation policy). This is where crawl.js wants to contribute. Building a high performance crawling architecture capable of downloading thousands of pages per second is a rather practical problem and relatively few work has been done on this specific topic \cite{ubicrawler}\cite{hp_crawler}. Additionally, at the time of writing, there is no crawling architecture that is distributed and locality-aware\footnote{Crawlers should fetch pages near them (in terms of latency)} that we know of.
\newline
Crawl.js is distributed, decentralized and locality-aware.

\section{Node.js \& Redis}
We built Crawl.js using the JavaScript environment Node.js\footnote{\url{http://www.nodejs.org}}. As quoted: Node.js uses an event-driven, non-blocking I/O model that makes it lightweight and efficient, perfect for data-intensive real-time applications that run across distributed devices. Regarding that a web crawler is data driven and resource intensive, using such an environment seemed a good choice. Additionally there is no other web crawler that we know of running on Node.js and making intensive use of redis\footnote{\url{http://redis.io}} to manage URLs.

\section{Outline}
In Chapter~\ref{Chapter2} we summarize the related work in the research domain of our thesis. Afterwards we discuss more in detail the specific problems we want to contribute to in Chapter~\ref{Chapter3}. The main chapters Chapter~\ref{Chapter4} and Chapter~\ref{Chapter5} give insights in how crawl.js works and how it performed during our experiments. Finally we come to some conclusions in Chapter~\ref{Chapter6} and mention some ideas for the future of crawl.js.
