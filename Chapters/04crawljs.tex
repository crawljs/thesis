
\chapter{Crawl.js} % Main chapter title
In this chapter we describe how crawl.js splits and distributes the work in detail.
\label{Chapter4} 
\lhead{Chapter 4. \emph{Crawl.js}} 

\section{System overview}
Crawl.js is a decentralized system. Each \emph{worker} in the system is standalone and autonomous and does not depend on any other worker. The only thing a worker needs is a connection to the \emph{Queues} and the \emph{Stores}. Figure~\ref{system_overview} gives an overview of the main components in the system.

Lets follow the path of an URL in the system.
The first thing a worker needs to do is get some URLs (1) from a remote queue (Note that the worker knows by his own which Queue to ask). Step 2 and 3 involves fetching, parsing and storing the content given by the URL. Thanks to the asynchronous nature (Streaming) of Crawl.js the parsing (3) can happen before the fetching (2) is done. Even storing the content (3) is done while fetching. More on this later. Whenever a new URL is extracted in the parsing step it is added to the \emph{Queue}. The queue then decides what to do (\emph{4a},\emph{4b}). Either it is kept in the local (in-memory) queue or dispatched to a remote one (batch processing).

\begin{figure}[h]
\centering
  \includegraphics[width=1\textwidth]{Figures/system_overview.pdf}
\caption{Crawl.js - System Overview}
\label{system_overview}
\end{figure}

\section{Mapper}
\label{mapper}
The mapper is not a physical component in the system like the \emph{Queue}, the \emph{Worker} or the \emph{Store}. It is a algorithm allowing each component at any time to map an URL to an identifier. This identifier is then primarily used to determine the responsible \emph{Queue} or \emph{Store} for that URL.
This ability of being able to compute an identifier for a URL \emph{independently} is key to the whole system architecture. This ability is what makes the system decentralized. We don't have to ask a central component with manager like capabilities. We can compute the identifier ourself. Basically it identifies a virtual block within the URL space (all URLs). Every URL belongs to one and only one virtual block and each block is:
\begin{itemize}
  \item Mutual exclusive - Each block belongs to one and one worker only.
  \item Equally sized - From block to block the number of URLs should roughly be the same.
\end{itemize}

In other words the mapper is responsible to \emph{split} the work. In Crawl.js the mapper is nothing more than a function implemented as a separate module. Each component that needs to be able to map an URL to an identifier just include the module. The algorithm used is rather simple and illustrated below:
\begin{algorithmic}[0]
\Function{Map}{$url$}\Comment{Map the URL to an identifier}
\State $host \gets url.host$\Comment{Only the host part ex. www.unine.ch}
\State $hash \gets \textbf{hash}(host)$\Comment{Hash the host part}
\State $id \gets hash \bmod n$\Comment{id $\in \{0,1,..n\}$, n:total number of virtual blocks}
\State \textbf{return} $id$
\EndFunction
\end{algorithmic}

So the idea is to hash the host part of the URL modulo the total number of blocks we want. The modulo number (total number of blocks) must be shared among every component using the mapper in the system.
To achieve this shared knowledge we can use a \emph{static} or a \emph{dynamic} approach. Using the static one is much simpler and achievable through simple configuration values. The key about the \emph{static} approach is that the number doesn't change during runtime. If we want to change the configuration, we have to shutdown all workers, reconfigure them and restart. Using the \emph{dynamic} approach would allow us to change the value while all workers continue to run. They would simply start mapping URLs to new identifiers as soon as we change the value. It is obvious that for the dynamic approach some sort of communication between the mappers is needed to interchange the newly assigned value of "total number of virtual url blocks".

\subsection{Politeness}
To start, what does it mean for a crawler to be polite. There are different aspects of politeness like the robots exclusion protocol (discussed in \ref{worker}) that we don't want to discuss here. We want to discuss how often a crawler should access a server within a time frame. It is not especially the number (ex. once every 30 seconds) that is important but the fact that we need to be able to limit/control this number.

In order to do that we hash only the host part of the URL in our mapping function. Doing so guarantees that there is always only one worker responsible for one host. Having this situation allows us now to control this aspect of politeness.
During our first test runs we accidentally hashed the whole URL. This resulted in an uncontrollable situation because multiple workers were fetching the same host simultaneously.

\subsection{Alternatives}
The introduced mapper function here is rather simplistic. Throughout the theoretical sections of this thesis we will reference the here described mapper implementation. But in section~\ref{section:experiments} we implement an alternative mapper and make use of it. It introduces the notion of groups to crawl.js and makes the configuration of crawl.js much more powerful. Important to note is that implementing a novel mapper approach is done easily and with more experiments even more mappers can be evaluated.

\section{Queues}
Queues play a central role in the Crawl.js system. There are \emph{local} queues and \emph{remote} queues. Each worker keeps a local, in-memory queue during his crawl for various reasons discussed in section \ref{queues_local} and each worker \emph{consumes} URLs from a remote queue. A remote queue represents one virtual URL block (as discussed in section \ref{mapper}). But, before we start with the details we state more precisely the term queue used in Crawl.js.

\subsection{Queue}
First a few words about how we use the word \emph{Queue} within this document. Strictly and technically speaking it is \emph{more} than a queue. A basic queue has the following operations:
\begin{itemize}
  \item enqueue
  \item dequeue
\end{itemize}

New items are enqueued at the bottom of the list and existing items are dequeued from the top. In terms of URLs this could work. But, what if we want to prioritize the URLs in the queue in some way. Of course we could enqueue URLs in the order we want them prioritized but this is rather inconvenient and does not work for existing URLs in the queue. We want to be able to re-prioritize (remove/change) existing URLs in the queue. And as mentioned in \cite{hp_crawler} fetching URLs in the order they were enqueued is a bad idea because of second-level locality in the links.

Therefore in Crawl.js a queue is a \emph{sorted set}. With sorted sets we can achieve whatever order we like. We can simulate a queue with it (thats why we still call it queue), use a pure random approach (set) or use custom algorithms to compute the order in the queues (re-visit policies).

\subsection{Consuming \& Producing}
The \emph{consumer} is the worker. Only one worker. So, the worker needs to know from which remote queue to consume from. This information is given as startup parameter. The parameter is not only the identifier of the queue but the identifier of the virtual URL block this worker is responsible for. We discussed the ability of every worker being able to map an URL to an id in section \ref{mapper}. With the help of this additional parameter the worker is now able to decide if he is responsible for a given URL (compare the ids). If he is the URL is added to the \emph{local} queue. If he is not the URL is added to appropriate \emph{remote} queue. Doing so makes the worker also a \emph{producer}. Obviously because there are multiple workers acting the same, we have a multiple producer situation for a single remote queue. 

\subsection{Local Queue (in-memory)}
\label{queues_local}
As the title describes it, from the worker's point of view it is a local and in-memory queue. The reason why every worker has a local queue is twofold:
\begin{itemize}
  \item We want the worker to be autonomous for a certain time. Therefore he needs a place to put the newly found URLs (during his crawl).
  \item Additionally we don't want to fetch the same URL twice (during one crawl of one worker). Therefore the local queue is the first filter used to eliminate duplicated URLs. Because the Queue is implemented as a sorted set, checking for the existence of an URL is easy an achievable in the complexity of O(1).
\end{itemize}

One thing to note is that the state of this local queue is in memory only. This means that when the worker crashes this state is lost. So we need to find a trade off between autonomy and fault tolerance. We want the worker to be autonomous (no inter communication) as much as possible, but on the other side we don't want to loose to much information if a worker crashes. Currently this trade off is adjustable with the help of a configuration value that limits the queue in size. Once this limit is reached, the worker does not put any new URLs in the local queue but dispatches them to the \emph{remote} one. This allows him to empty his queue and finish his crawl. An additional idea would be to limit a crawl in terms of time, but this is not implemented yet.

\subsection{Remote Queue}
\label{queues_remote}
As mentioned in the introduction of this section one worker is responsible to empty his queue and all other workers are able to fill it. The state of all remote queues represents the whole system state. URLs on the top of those queues are going to be fetched next and URLs at the bottom have just been fetched successfully. It is obvious that this state is important and need to be persisted. 


\subsection{Remote Queue implementation (Redis.io)}
In Crawl.js remote queues are implemented with Redis\footnote{Redis - \url{http://www.redis.io}}. We use one sorted set for one remote queue. Luckily the sorted set from Redis gives us all the atomic operations we need. Redis uses the notion of score to sort the elements in the set. With the help of this score value we can not only sort the elements within the queue, but we can also store state information with it (ex. score > 0 := FETCHED, score < 0 := FETCH). Each element in the sorted set is a tuple (member,score) where the member is an URL in our case. How the worker uses the score together with the available commands is described below. Low (even negative) score values represent URLs that are going to be fetched next.

\begin{enumerate}
  \item \textbf{ZRANGEBYSCORE}\footnote{\url{http://redis.io/commands/zrangebyscore}} (single consumer) - get next URLs that need to be fetched. Note that the elements in the queue are not removed yet.
  \item \textbf{ZINCRBY} (multiple producers) - decrement the score of URL by a constant value. This happens when workers dispatch extracted URLs. It makes the entry move up the queue.
  \item \textbf{ZADD} (single producer) - As soon as an URL is fetched successfully we set the score to the current time stamp. This makes the entry move to the bottom of the queue.
\end{enumerate}

This is maybe not the best approach of how to prioritize URLs within the remote queues, but it is was simple to implement and allowed us to run first tests. The important thing to note here is that we have set a base with sorted sets that allows us to easily try out new approaches. As an example we could introduce an independent component in the system that takes care of computing the order of our queues based on some algorithms. (ex. Page rank).

\section{Stores}
Besides \emph{Queues} there is another important component in the system. \emph{Stores}. While queues persist the system state, stores persist the downloaded content (html, css, js, ..). The worker is able to identify his store using the \ref{mapper} module.
\subsection{Consuming \& Producing}
In this component we do not have \emph{consumers}, We have \emph{producers} only and because a worker is producing to his store only, there is no multiple producer situation for a single \emph{store}.

\subsection{Store}
The \emph{store} is here to persist the downloaded data of one worker. The API towards a store looks like this:
\begin{itemize}
  \item PUT (URL, DATA, [timestamp])
\end{itemize}
This abstraction gives us the ability to implement different store engines and encapsulate implementation details (batch-processing) in a modular way. Like the sorted sets in the queues, the store gives us the flexiblity to do whatever we want with the downloaded data. In Crawl.js we have implemented different store engines:
\begin{itemize}
  \item Riak, Cassandra - NoSql
  \item Fs - Local Filesystem
  \item Dummy - Does nothing
\end{itemize}

\section{Worker (a Crawl.js instance)}
\label{worker}
While Crawl.js is the whole system composed of different components (queue, store, worker) working together, a Crawl.js instance is a single worker. This worker is implemented in JavaScript and runs on Node.js\footnote{Node.js - \url{http://nodejs.org}}.
In this section we will talk about the Software Architecture and discuss some implementation details.

\subsection{Node.js}
What we really wanted is native support for non blocking IO because crawling webpages involves a lot of IO. We don't want to block while waiting for a response to be completely available (headers, body). We want to fire requests and be notified (called back) when thre is some data available. And while we are idleing we have time to do other things (parsing). In Node.js everything is done through events (callbacks). An event is a function that gets executed with a defined context. It starts with the initial start script and ends when there are no more callbacks to be executed. This event model fits very well and allowed us to implement the crawler in a elegant way. We didn't need to struggle with multiple Threads and possible dead locks to achieve concurrency. Below are some example events we will encouter throughout the code.

\begin{itemize}
  \item on 'url': fetch page
  \item on 'chunk': extract <a> tags, ..
  \item on 'chunk': store it, batch process it, ..
  \item emit 'url': from extraced <a> tags
  \item ...
\end{itemize}

As you can see the event model even allows us to stream data (No need to wait until everything is downloaded). As soon as we get some response data we can start to do things. Note the plural.

\subsection{Directory Layout}
As mentioned above Crawl.js is written in JavaScript. In this section we will start with a directory layout to get an overview of the code. Afterwards we will explain the most important code snippets more in detail and follow to the various events throughout the application.
\\
\dirtree{%
  .1 /crawl.js\DTcomment{Project root}.
  .2 benchmarks\DTcomment{Some benchmarks (parsing, resolving links)}.
  .2 config.json\DTcomment{Configuration file}.
  .2 crawl.js\DTcomment{Application entry point}.
  .2 lib.
  .3 config.js\DTcomment{Utility to load/cache config files}.
  .3 dispatcher.js.\DTcomment{Dispatch/discard found URLs}
  .3 extractor.js\DTcomment{Extractor interface}.
  .3 extractors.
  .4 parser.js\DTcomment{Extractor using a SAX like parser}.
  .4 regex.js\DTcomment{Extractor using regular expressions}.
  .3 fetcher.js\DTcomment{Concurrent request/response handling}.
  .3 logger.js\DTcomment{Simple stdout logging}.
  .3 mapper.js\DTcomment{Maps URLs to blocks (\& groups)}.
  .3 queue.js\DTcomment{Queue interface}.
  .3 queues.
  .4 memory.js\DTcomment{in-memory implementation (local queue)}.
  .4 redis.js\DTcomment{Implementation using redis (remote queue)}.
  .3 queues.js\DTcomment{Access queues easily}.
  .3 robo.js\DTcomment{Robots exclusion protocol with caching capabilities}.
  .3 seen.js\DTcomment{Set to keep track of seen URLs}.
  .3 store.js\DTcomment{Store interface}.
  .3 stores.
  .4 cassandra.js\DTcomment{Cassandra implementation}.
  .4 fs.js\DTcomment{Local filesystem implementation}.
  .4 \ldots.
  .3 url.js\DTcomment{URL utilities}.
  .2 node\_modules\DTcomment{Node.js dependencies (3rd party libraries)}.
  .2 package.json\DTcomment{npm configuration https://npmjs.org/doc/json.html}.
  .2 tools.
  .3 importer.js\DTcomment{Import some initial URLs into the queues}.
}

\subsection{Application (crawl.js)}
Lets start with the entry point. To run a worker we need to provide one argument. The id of the virtual URL block (url-block). A worker needs to know for which block he is responsible for. The range of the available url-blocks is configurable in \emph{config.json}.

\begin{lstlisting}[language=sh]
$ node crawl.js <url-block>
\end{lstlisting}

The first function executed is \emph{init}.

\begin{lstlisting}[language=JavaScript]
function init(block) {

  conf.block = parseInt(block, 10); //virtual url-block passed as argument on startup
  queues.local().on('url', crawl); //establish event flow
  fetcher.init();

  peek(); //get urls from remote queue

}
\end{lstlisting}

First, we populate (Line 3) the virtual url block we are responsible to the whole application and establish the event flow. After that we peek (without removing them) some URLs from the remote queue and as soon as we get some URLs back the 'url' event is fired which causes the crawl to start. Also later during the crawl, whenever a new URL is found (ex. parsing) it is the same event that gets fired over and over. So lets examine the function \emph{crawl} that gets triggered by the 'url' event (Line 4).

\begin{lstlisting}[language=JavaScript]
function crawl() {

  if (fetcher.isBusy()) {
    //max number of concurrent connections reached.
    return;
  }

  var queue = queues.local()
    , url = queue.dequeue();

  if (!url) {
    if (!fetcher.isActive()) {
      if (quitting) {
        quit();
      } else {
        log.info('job done! waiting to restart.');
        setTimeout(peek, 5000);
      }
    }
    //other crawlers still running. they will trigger more events
    return;
  }

  fetcher.get(url, function (err) {
    if (err) {
      log.error('fetch went wrong: %s', err);
    }
    printQueue();
    process.nextTick(crawl);
  });

}
\end{lstlisting}

To be able to crawl a page we need the \emph{Fetcher} in the first place. So if he is busy we do nothing. Busy means that the fetcher has reached the configured value (config.json) of concurrent connections.
Lets assume the fetcher is not busy and we have an URL in our queue. In that case, we are going to crawl our first page. The URL is dequeued and passed to the fetcher (Line 24). Together with the URL we pass our callback function to continue the event flow. This is a typical example of the asychronous programming style in Node.js. The passed function (callback) is called as soon as the fetcher is done. That means we return immediately from the \emph{crawl} function and while the fetcher establishes the connection to our URL and begins to download the content we do not waste CPU resources. 

Lets take a closer look to the passed callback function in Line 24. There is one argument: \emph{err}. It is a convetion in Node.js that the first argument of a asynchronous callback function is always the error. Even if there is none. We will encounter this programming style through out the code but now lets focus on what gets done when our callback is executed. In case of an error we log it and for debugging reasons we print out our local queue. But the most important thing happens on Line 29. Here the event loop of Node.js is presented to us as a language construct. It looks like a recursive call of \emph{crawl} but in fact we tell the event loop (behind the scenes, vm) to include the callback \emph{crawl} in his execution loop. Doing so makes sure that our worker never terminates.

\subsection{Fetcher (fetcher.js)}
Next thing to look at is the fetcher. We mentioned before that we are able to process multiple concurrent connections without multiple threads. So how is this actually achieved and how does the event flow continue after we get some data back.
Lets start with the most important function \emph{get}. Note that this is a class function.

\begin{lstlisting}[language=JavaScript]
Fetcher.get = function (url, cb) {
  if (Fetcher.isBusy()) {
    return cb(new Error('Fetcher is busy'));
  }
  Fetcher.active++;
  Fetcher.instances[Fetcher.next++].get(url, function () {
    Fetcher.active--;
    cb.apply(this, arguments);
  });
  if (Fetcher.next >= Fetcher.instances.length) {
    Fetcher.next = 0;
  }
};
\end{lstlisting}

What this function does is assigning the job of getting the URL to an instance [Line 6]. As you can see there are multiple instances and the selection is done in a circular fassion (round-robin). Actually this is how we achieve multiple concurrent connections with a single thread. Because assigning one of the instances is done asynchronously (providing a callback Line 6) we can immediately return out of the function. The assigned instance establishes the connection (discussed later) which involves a lot of waiting and because in Node.js nothing blocks during this time \emph{get} can be executed again. (Assigning another instance). Once all instances are assigned we end up in a busy fetcher. In this situation we have reached our max. number of concurrent connections which is equal to the number of instances (configurable in config.json).

Because our class function \emph{get} does only assign jobs to fetcher instances, lets have a look at a code snippet of an instance.

\begin{lstlisting}[language=JavaScript]
Fetcher.prototype.get = function (url, cb){
  ...
  //happens before any 'data' event
  req.on('response', function (resp) {

    var options = resp.headers || {}
      , ct = resp.headers['content-type'] || '';

    self.extractor.setBaseUrl(self._url);//the URL we are currently fetching

    //parse only content-type text/* with statusCode 200
    if (resp.statusCode === 200 && (ct.indexOf('text/') >= 0 || ct === '')) {
      req.pipe(self.extractor, {end: false});
    }
    ...
  });
};
\end{lstlisting}

We intentionally left out the details of setting up the request (timeout, max. redirects, ..) because we want to focus on those lines. It is where the event flow continues. That is to say we \emph{pipe} our Readable stream \footnote{\url{http://nodejs.org/api/stream.html}}, the request, into our \emph{extractor} (discussed later). The concept of a pipeline is nothing new and a very powerful tool in the Unix world. "A set of processes chained by their standard streams, so that the output of each process (stdout) feeds directly as input (stdin) to the next one"\cite{wiki:pipeline_unix}. Our first element in the pipe (the request) feeds the extractor with multiple \emph{data} events. The fact that we can emit those events as soon as we receive them from the network (chunks) is very handy because the extractor can start extracting links while we are waiting for more \emph{data}. This event based IO (asynchronous) is what makes Node.js so performant while remaining relatively easy to use.

Lets have a look at the consumer.

\subsection{Extractor (extractor.js)}
This component is responsible for extracting new URLs out of the downloaded document (html). The newly found URLs are passed to the \emph{dispatcher} who's goal is to filter out URLs based on various rules (see dispatcher). As we mentioned above we pipe \emph{data} events into the extractor, so obviously the extractor has to be a writable stream\footnote{\url{http://nodejs.org/api/stream.html}}. Additionally we have implemented the extractor in such a way that the actual work of extracting the URLs is done by an engine. Currently there are two implementations available:
\begin{itemize}
  \item Regex - Extractor engine based on regular expressions
  \item Parser - Extractor engine based on a SAX like parser
\end{itemize}
Additional engines can be added easily and which one to use is configured in config.json. Lets have a look at the consumer side of our pipe, the function \emph{\_write} a writable stream must implement.

\begin{lstlisting}[language=JavaScript]
Parser.prototype._write = function(chunk, enc, cb) {
  this.parser.write(chunk);
  cb();
};
\end{lstlisting}

It does nothing but forward the data to the parser and signal (Line 3) the first element in the pipe (the request) that we successfully handled the data. During the parsing we call the function \emph{Extractor.prototype.found(urlObj)} whenever we find an URL we are interested in (ex. <a> tags). See below the very simple implementation of it.

\begin{lstlisting}[language=JavaScript]
Extractor.prototype.found = function (urlObj) {
  this.dispatcher.dispatch(urlObj);
};
\end{lstlisting}

\subsection{Dispatcher (dispatcher.js)}
Actually we do not only dispatch URLs. We discard unwanted URLs and normalize the accepted ones. The topic of efficient URL normalization\cite{wiki:url_normalization} is very complex and our approach does not respect any aspects of this research area at all. The implemented strategy is way too simple and may be revised in future versions of crawl.js.

After normalizing the accepted URL we dispatch it the following way:

\begin{lstlisting}[language=JavaScript]
Dispatcher.prototype.dispatch = function(urlObj) {
  ...
  var entry = url.normalize(urlObj)
    , block;
  
  if (!self.seen.isMember(entry)) {
    self.seen.add(entry);
    block = self.mapper.block(urlObj);
  
    if (self._forUs(block) && !Dispatcher.blocked) {
      if (!self.localQueue.enqueue(entry)) {
        Dispatcher.block(true);
      }                                                                                                                                
    }
    //to keep local & remote queue in sync
    self.remoteQueue.enqueue(entry, block);
  }
};
\end{lstlisting}

If the identifier of the URL (Mapper [\ref{mapper}]) matches the virtual URL \emph{block} we are responsible for we enqueue the URL to our local queue. Additionally we enqueue it remotely in order to keep the local and remote queues in sync. We do this to survive worker crashes. If it happens we can restart - syncing our local queue with the remote one - and continue where we left. Additionally we stop to enqueue to the local queue as soon as it is full (limit configurable in config.json). This configurable limit allows us to tweak the trade-off between fault tolerance and autonomy as mentioned in section \ref{queues_local}. Setting a low value (ex. 100) makes the worker end after a short period of time and re ask the remote queue for new URLs (less autonomous). Respectively, a large value makes the crawl more autonomous.

\subsection{Seen (seen.js)}
In the last snippet an important module was not discussed. The seen module. It is pretty simple in terms of functions it provides:
\begin{itemize}
  \item isMember (entry) - check `entry` for existence
  \item add (entry) - mark `entry` as seen
\end{itemize}
But still very important for a worker and his autonomy. Designing and implementing a high performance seen module, able to deal with billions of entries is a complex topic and can not be done in memory only for obvious reasons \cite{hp_crawler}. In crawl.js we do not have to deal with a seen module capable of such properties. Because Crawl.js is decentralized, the seen module is too. And as such the number of entries we have to deal with is much smaller (1-10 Millions). Basically it is equal to the size of the URL block we are responsible for. Of course keeping URL blocks reasonably small is left to the configuration but in our implementation we assumed that it is the case.
\newline
\newline
Therefore our seen module is backed by a JavaScript Object - which is a set - and is in memory only. This allows us to test an URL for existence locally and therefore very efficiently O(1). Additionally the seen module is initialized with the URL entries found in the remote queue (only the downloaded ones). With this, the memory state can be re-initialized after a worker crash for example. 

\subsection{Queue (queue.js)}
We started the crawl by asking the \emph{remote} queue for URLs and we end it when there are no URLs left in the \emph{local} queue. Queues are key components to crawl.js and a good and performant implementation of them is very important. Our current approach is to have two queues: a \emph{local} and a \emph{remote} one. They are easily accessible through the helper module \emph{queues.js}. But both implement (parts of it) the queue interface we defined in \emph{queue.js}. To keep things identical, queue implementations are also called \emph{engines}. Currently we have two engines available:
\begin{itemize}
  \item Memory - volatile implementation using an array
  \item Redis - implemented using a redis server as backend
\end{itemize}
Which queue engine to use is configurable in \emph{config.json}. This setup (using engines) allows us to add new queue implementations easily. As an example lets consider the following idea for a remote queue using redis: 2 sets, one for newly found URLs, one for URLs we have successfully downloaded. We could easily implement:
\begin{itemize}
  \item enqueue - add to the set of new URLs (redis sadd\footnote{\url{http://redis.io/commands\#set}})
  \item dequeue - add to the set of downloaded URLs (redis sadd)
  \item peek - substract set of downloaded URLs from set of new URLs (sdiff)
\end{itemize}
This idea could easily be implemented as a seperate engine (ex. redis-two-sets.js).

\subsection{Configuration (config.json)}
We referenced the ability of various configurable parameters of a worker. You find the configuration of experiment 3 in ~\nameref{appendix:config.json}. The keys are pretty self explaining.

\subsection{Repository}
Our project is available at github - \url{https://github.com/crawljs} - any contributions are very welcome.
