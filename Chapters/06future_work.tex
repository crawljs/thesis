% Chapter Template

\chapter{Conclusions and future work} % Main chapter title
\label{Chapter6} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}
\lhead{Chapter 6. \emph{Conclusions and future work}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title

Crawl.js is not ready to be let out into the wild. Far from it. It misses many features a mature crawler should have which we discuss in this section. But the focus of Crawl.js was notably to design a distributed crawling architecture able to download sites in parallel and at high speed. An architecture that allows us to crawl sites efficiently and is able to scale easily as needed. Furthermore we wanted an architecture that we can deploy on different (small) data centers in order to be as \emph{close} (in terms of latency) as possible to the sites we fetch. We intentionally left out other important aspects of web crawling such as indexing/searching, page importance, selection policy and revisit policies as many research has been done on those topics already \cite{page_importance1}\cite{page_importance2}\cite{wiki:web_crawler}. Nevertheless, we did design Crawl.js in such a way that features regarding those topics can be added easily some time later.
\newline
\newline
With this in mind we are pretty satisfied with the experiences we gained throughout our work and with the final results we achieved. The Node.js environment proved itself to be a perfect fit for the implementation of Crawl.js. Because of its event-driven model we could keep our code clean and simple. Not only did we never struggle with concurrency-/deadlock situations but the purely asynchronous nature of it granted other advantages:
\begin{itemize}
\item Resource capacity: By tuning the number of concurrent connections we achieved to operate at almost full capacity (~90\% CPU) easily.
\item Good performance: Around 250 pages/second per worker. Especially when taking into account that we parse the downloaded pages too. You find details about the hardware used for one worker in Appendix~\ref{appendix:worker}.
\item Integration with Redis.
\end{itemize}
Working with redis and using its data structures to manage our queues was another great decision we made. The set (sorted-set) structures gave us all the atomic operations we needed and performed very good in our simple experiments. Of course a cluster must be set up as the number of URLs grows to share the key space among multiple servers but for Crawl.js nothing changes if using a cluster or a single server. Furthermore, is the ability of distributing the key space to different servers (data-centers) a feature that the Crawl.js system highly values. Remember we wanted an architecture deployable to different (small) data centers.
\newline
With the help of the set (sorted-set) structures we could also easily introduce groups of workers in order to respect \emph{closeness} to the pages we fetch. Groups (and blocks) are nothing more than (set-)keys of the following form: 'group:XX:block:YY'. And because the redis commands used to manipulate the sets (sorted-sets) are mostly able to deal with multiple values (members) at once, we were able to implement a simple batch processing of URLs very easily. This made the communication between the workers and redis very efficient.
\newline
Last but not least we achieved an almost linear scaling behaviour, which in our eyes is a pretty good foundation to build upon.

\section{Future work}
Here are some ideas how Crawl.js could evolve in the future. They came across during our work and could not be implemented mostly because of time/focus constraints. The order is purely random and has nothing to do with our preference.

\subsection{Dynamic configuration}
In Crawl.js the configuration is done statically. We define the number of groups (and the number of blocks within them) in a file \emph{before} we start the crawler. In our simple experiment setup this was good enough. But the need to this during \emph{runtime} is obvious. Especially when crawling the public internet (or part of) the number of workers needed (to finish in a reasonable time) is not known in advance. Neither how to distribute them to the different groups, or how many groups are needed at all. Therefore a configuration/monitoring channel is really needed. Through this communication channel the workers would \emph{announce} them self and request a valid configuration to start crawling. Of course new configurations could be pushed during runtime too. Additionally monitoring functionalities could go through this channel too which leads us to the next section. 

\subsection{Administration}
Making sure that all workers (and other components) are alive and healthy is a need that becomes more and more important as the number of components within the system grows. As an administrator of the system we need to \emph{monitor} the components and have some sort of an interface to take appropriate \emph{actions}.

\subsection{Blacklist}
The management of a global blacklist is just an example of an important administration task. When crawling the public internet we need to be able to react to the following example situations:
\begin{itemize}
  \item Rescue workers from spider traps \cite{wiki:spider_trap}.
  \item React to complaints about our crawler.
\end{itemize}

\subsection{Public crawl}
We did mention that crawling the public internet would be the next step to really test Crawl.js. In order to do that a good supervision is essential. Especially handling complaints quickly in order not to ruin the reputation of Crawl.js. When designing and implementing a crawler errors such as crawling a site too often are inevitable and therefore a good administration is essential. Therefore we suggest to implement an administration interface \emph{before} doing a significantly big public crawl.

\subsection{URL normalization}
The topic of URL normalization \cite{wiki:url_normalization} was absolutely not respected during our implementation. Future versions of Crawl.js should consider implementing at least the discussed approaches in RFC3986 \cite{rfc:url_normalization}.

\subsection{Page importance}
Page importance and related graph problems have not been taken into account. The order at which Crawl.js fetches pages is random. While the topic of page importance is complex it could be implemented easily in Crawl.js, mainly because Crawl.js uses distributed (sorted-) sets to manage URLs. With this data structure a PageRank\cite{google} could be computed off-line and the resulting sorted set used by the workers. Or maybe a novel on-line algorithm such as OPIC (On-line Page Importance Computation) \cite{page_importance1} could be implemented as well.
