% Chapter Template

\chapter{Experiments and results} % Main chapter title

\label{Chapter5} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{Chapter 5. \emph{Experiments and results}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title

After discussing how crawl.js works we want to run some experiments. We decided to test crawl.js in a closed environment. Crawling the public internet requires a good supervision and the required work to do that is simply out of scope for this thesis. All experiments were done on the opennebula cluster hosted at the university of neuchatel.
\\
First we describe our \emph{setup} and afterwards we discuss the different experiments we ran on it. \emph{Experiment 1} focuses on verifying basic properties of crawl.js (e.g. scaling). In \emph{Experiment 2 \& 3} we introduce latency and compare two different \emph{mapper}~\ref{mapper} approaches. A simple (random,hash) URL to worker assignment and a 2-level assignment.


\section{Setup}
To do our experiments we had to setup the following types of VM in the cluster:
\begin{itemize}
\item www - static wikipedia snapshots.
\item redis - remote queues.
\item workers - crawls pages from \emph{www} and updates remote queues in \emph{redis}.
\end{itemize}
All VM's are based on a ubuntu server 12.04.3 LTS (64bit) image.

To setup and manage (workers) the VMs efficiently we wrote some basic bash scripts. It allowed us to deploy different worker configurations easily and perform basic operations on the workers such as start and stop. Having those scripts saved us a lot of time throughout the different experiments we ran. You find all scripts used throughout our experiments in the crawl.js git repository. In Figure~\ref{test_setup} is an overview of all the VMs used during the experiments.

\begin{figure}[h]
\centering
  \includegraphics[width=1.0\textwidth]{Figures/experiments_setup.pdf}
\caption{Crawl.js - Test setup overview}
\label{test_setup}
\end{figure}

\subsection{www - wikipedia snapshots}
In order to have realistic sites to crawl we decided to setup wikipedia html dumps. Unfortunately their dumper stopped back in 2008 and therefore the snapshots are pretty old. But for our use case it is good enough. Almost. Back then, the different wikipedia languages were located in different subdirectories. All on the same server. But we need links pointing to different servers to be able to add latency to some of the sites and experiment with the \emph{closeness} aspect of crawl.js. Therefore we wrote an xsl transformation script to change all language links from /de/index.html to de.wikipedia.org/index.html.


Additionally we encountered a serious performance problem during our first tests. The random read performance in our VM was about 200 IOPS (tested with fio). Therefore the web server (nginx) could not deliver pages fast enough and represented the bottleneck in our setup. In order to circumvent this unwanted side effect we put our html dumps on a ramfs.

\begin{itemize}
  \item CPU: 1, VCPU: 1
  \item RAM: 2048M
\end{itemize}

\subsection{redis - Remote queues}
This setup was pretty simple. Just a single redis server instance running. Because all the URLs have to fit in memory, setting up a redis cluster to share the key space on different servers is inevitable as soon the number of URLs becomes too big. In our simple/small setup this was not needed. During our experiments we encountered 217'690 URLs which represents 82M of data inside the redis server (redis info command).

\begin{itemize}
  \item CPU: 1, VCPU: 1
  \item RAM: 3072M
\end{itemize}

\subsection{worker - Crawl.js worker instance}
The worker VM hosts our developed crawler (crawl.js instance). Setting it up was straight forward. First we thought that we could host more than one crawler instance (using multiple cpus) but we achieved better results with one crawler per worker. This 1:1 worker to crawler relation made the management easier too.

\begin{itemize}
  \item CPU: 1, VCPU: 1
  \item RAM: 1024M
\end{itemize}

\section{Experiment 1}

The goal of our first experiment was to verify the following basic properties of crawl.js.

\begin{itemize}
  \item The crawl stops
  \item Adding more workers reduces the crawl duration (1, 2 \& 4 workers)
  \item All pages are found (starting with a single seed URL)
\end{itemize}

\subsection{Configuration}

Here is an overview of the configuration we used.

\begin{itemize}
  \item Mapper: simple (random URL to worker assignment)
  \item Workers: 1, 2 \& 4
  \item Sites: http://bn.wikipedia.org, http://bs.wikipedia.org (142'082 articles)
  \item Site latencies: None
\end{itemize}

\subsection{Analysis}

 All tests (1, 2 and 4 workers) started with the same seed URL (http://bn.wikipedia.org) and as shown in Figure~\ref{plot:exp_001} the first results verified both properties: \emph{The crawl stops} and \emph{Adding more workers reduces the crawl duration}. The scaling behaviour actually exceeded our expectations as it was almost linear. The communication overhead needed to perform a crawl with more than 1 worker was minimal. A possible explanation for this overhead is worker idle times. Because every worker flushes the newly found URLs only once every 10 seconds (configurable), other workers could ran out of work and be forced to wait (and re ask later). Verifying this assumption would require an additional experiment. Basically exactly the same experiment but starting with many seed URLs (those found during this experiment for example). We are pretty sure that with a big seed list idling would not happen or at least be shorter.
\newline
\newline
For the last property \emph{All pages are found} wikipedia provides a downloadable text file containing all article URLs. Using this file we were able to import the expected URLs into redis (set datastructure). Afterwards we took the difference (redis diff) between the two sets 'urls:expected' and 'urls:crawled'. Unfortunately the resulting set was not empty. It had 23'338 URLs. That number seemed us pretty huge, thats about 16\% of all articles. Did we miss that many URLs? We decided to have a closer look at some randomly picked articles to check what we actually missed. We discovered quickly that most of these 'missed' articles redirected (alias) to other articles we did crawl (13'391 redirect articles). Because articles never reference articles through an alias, crawl.js did not discover those 'redirect-articles' (as we follow references,<a> tags). This still leaves us with 9'947 (7\%) of potentially missed articles. Again we randomly picked some of the remaining articles and discovered that none of them were referenced by other articles (using grep -r). Of course we did not check for references to all remaining articles but 7\% of orphaned articles~\footnote{\url{http://en.wikipedia.org/wiki/Wikipedia:WikiProject_Orphanage}} seemed us realistic and therefore the verification of this property good enough to proceed with our experiments.
\newline
\newline
\begin{figure}
\centering
  \begin{tikzpicture} 
    \begin{axis}[
      ybar,
      bar width=12pt,
      enlargelimits=0.2,
      axis y line*=left,
      xlabel={Workers},
      xlabel near ticks,
      ylabel={Minutes},
      ylabel near ticks,
      xtick=data,
      nodes near coords,
      nodes near coords align={vertical},
      ]
      \addplot
        coordinates {(1,13.78) (2,7.47) (4,4.07)};
    \end{axis}
  \end{tikzpicture}
\caption{Scaling behaviour (1,2 \& 4 workers)}
\label{plot:exp_001}
\end{figure}

\section{Experiment 2}

In this experiment we took the same components (4 workers) as in experiment 1. Because we implemented a novel 2-level mapper we first wanted to make sure it performs as good as the one used in experiment 1. Therefore we repeated the last test from experiment 1. Afterwards we changed the network topology. You can find a simple overview in Figure~\ref{test_setup2}. Basically we introduced two virtual regions by adding latencies between workers and wikipedia servers. The latency values used reflect the \emph{closeness} of the regions towards the wikipedia servers. With this new topology we want to demonstrate how drastically crawl performance suffers when dealing with latencies. Especially when using a random URL to worker assignment. In experiment 3 we will reconfigure the system in order to obtain better crawl performances with the same network topology in place.

\begin{figure}[h]
\centering
  \includegraphics[width=1.0\textwidth]{Figures/test_setup2.pdf}
  \caption{Crawl.js - Test setup experiment 2 \& 3}
\label{test_setup2}
\end{figure}

\subsection{2-level Mapper (Groups of workers)}
This novel mapper introduces the \emph{closeness} aspect. Closer sites (in terms of latency) should be crawled by workers near them. 
\newline
As the name suggests it, the URL to worker assignment is done using 2 levels. The first level (e.g. hostname) determines the group and the second level (e.g. path + query part) identifies the worker within that group. The 2-level mapper introduces the notion of groups to crawl.js and adds an additional dimension to the configuration. Previously we had 1 dimension: url-blocks. Now we have 2: groups \& blocks within that group. Note that each group can be configured independently. 1 group may have 1 worker and another 3 (e.g. depending on the number of URLs a group is responsible for).
\newline
With groups we have now the ability to configure crawl.js in such a way that \emph{closeness} is respected. For example we can create a group \emph{switzerland} responsible for crawling the top-level domain \emph{ch}. How different group configurations affects the total crawl duration will be shown in experiment 3.
\newline
In this experiment we used one group with 4 blocks to be as close to the configuration used in experiment 1.

\subsection{Configuration}
\begin{itemize}
  \item Mapper: 2-level
  \item Groups: 0: 4 blocks
  \item Workers: 4
  \item Sites: http://bs.wikipedia.org, http://bn.wikipedia.org
  \item Topologies: 1 (no latencies), 2 (2 regions, Figure~\ref{test_setup2})
\end{itemize}

\subsection{Analysis}
Exactly as in experiment 1 all tests started with a single seed URL. http://bn.wikipedia.org. In topology 1 (no latencies) we can see in Figure~\ref{plot:exp_002} that the novel 2-level mapper performs as good as the simple mapper (4m06s vs. 4m04s). The observed difference to experiment 1 is probably due to the cluster being more or less busy during our experiments. We repeated the test and measured 4m05s which enforces our assumption.
\\
Using topology 2 (Figure~\ref{test_setup2}) we can clearly see in Figure~\ref{plot:exp_002} that the crawl performance suffers drastically. In our simple setup the theoretical explanation for the measured values is straight forward. Note that we need to do it from the perspective of both regions. And because our crawl is not complete until \emph{both} regions are done our final result $D$ is the maximum out of both calculations. We will only illustrate the calculation of region 1, doing it for region 2 is analog. So, from the perspective of region 1 the calculation looks as follows:
\newline
\[ D_1 = \frac{n1}{c} dt_1 + \frac{n2}{c} dt_2 \]
\newline
\begin{itemize}
  \item $c:$ 20 ( 2 workers with 10 concurrent connections each)
  \item $n1:$ 130'966 (bs.wikipedia.org)
  \item $n2:$ 86'724 (bn.wikipedia.org)
  \item $dt_1:$ 100ms
  \item $dt_2:$ 200ms
\end{itemize}

For $D_1$ we get 25.38 minutes and for $D_2$ (region 2, switch $dt_1$ and $dt_2$) 29.05 minutes. Therefore the crawl delay $D$ is \emph{29.05 minutes}. As you can see in Figure~\ref{plot:exp_002} these calculations are pretty good in line with the measured values.

\begin{figure}
  \centering
  \[ D = \sum\limits_{k=1}^s \frac{n_k}{c} (dt_k) \]
  \begin{tabular}{@{}>{$}l<{$}l@{}}
    D & total crawl delay\\
    s & number of sites\\
    n_k & pages on site k\\
    dt_k & latency on site k\\
    c & Concurrent connections (constant)\\
  \end{tabular}
  \caption{Equation: Total crawl delay D}
  \label{eq:exp_002}
\end{figure}

\begin{figure}
  \centering
  \begin{tikzpicture} 
    \begin{axis}[
        ybar,
        bar width=12pt,
        axis y line*=left,
        enlargelimits=0.2,
        xlabel={Network topology},
        xlabel near ticks,
        ylabel={Minutes},
        ylabel near ticks,
        xtick=data,
        nodes near coords,
        nodes near coords align={vertical},
      ]
      \addplot
      coordinates {(1,4.1) (2,29)};
    \end{axis}
  \end{tikzpicture}
  \caption{Crawl duration T on different topologies}
  \label{plot:exp_002}
\end{figure}

\section{Experiment 3}

In the previous experiment we showed that latency on sites affects the crawl performance heavily. In this experiment we want to demonstrate that crawl.js is able to deal with different latencies on different sites. We want to show that when using the exact same latency configurations (100ms) as in experiment 2 we are able to achieve better performances by reconfiguring crawl.js. We will keep the number of workers the same but we will re-group them differently in order to respect \emph{closeness}.

\subsection{Configuration}
Test one exactly reflects the same setup as in experiment 2. 1 group with 4 workers and 100ms latency on bs.wikipedia.org. We repeat it just to confirm the measured results in experiment 2 and to prepare for the remaining tests. Because there is only one group and the URL to worker assignment is done randomly every worker is affected by the latency. In order to achieve a better performance we need workers that are not affected by the latency. Therefore we introduce a second group (Test 2 \& 3). One group is responsible for bs.wikipedia.org (100ms latency) and the other one for bn.wikipedia.org (no latency). With this setup we are be able to have workers that can operate on sites without (or with less) latency. We do 2 different tests with those 2 groups. First we keep the groups equally powerful by assigning 2 workers to each of them, second we rebalance the groups by moving 1 worker to the other group. This way we will have one group with a single worker responsible for bn.wikipedia.org (no latency) and another group with 3 workers responsible for bs.wikipedia.org (100ms latency). You can see the measured crawl durations in Figure~\ref{plot:exp_003}.
\begin{itemize}
  \item Mapper: 2-level
  \item Workers: 4
  \item Groups:
    \begin{itemize}
      \item Config 0: 1 group with 4 blocks
      \item Config 1: 2 groups with 2 blocks each
      \item Config 2: 1 group with 1 block, 1 group with 3 blocks
    \end{itemize}
  \item Sites: http://bs.wikipedia.org, http://bn.wikipedia.org
  \item Latency: 100ms on bs.wikipedia.org
\end{itemize}

\subsection{Analysis}

\begin{figure}
  \centering
  \begin{tikzpicture} 
    \begin{axis}[
        ybar,
        bar width=12pt,
        axis y line*=left,
        enlargelimits=0.2,
        xlabel={Group Configurations},
        xlabel near ticks,
        ylabel={Minutes},
        ylabel near ticks,
        xtick=data,
        nodes near coords,
        nodes near coords align={vertical},
      ]
      \addplot
      coordinates {(0,13.2) (1,25.05) (2,16.3)};
    \end{axis}
  \end{tikzpicture}
  \caption{Crawl performance using different group configurations}
  \label{plot:exp_003}
\end{figure}
