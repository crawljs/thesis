% Chapter Template

\chapter{Experiments and results} % Main chapter title

\label{Chapter5} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{Chapter 5. \emph{Experiments and results}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title

After discussing how crawl.js works we want to run some experiments. We decided to test crawl.js in a closed environment. Crawling the public internet requires a good supervision and the required work to do that is simply out of scope for this thesis. All experiments were done on the opennebula cluster hosted at the university of neuchatel.
\\
First we describe our \emph{measurement} techniques and how our \emph{setup} looks like. The \emph{setup} will stay the same during all experiments but we will use different network topologies (achieved with virtual latencies) during our work. In \emph{experiment 1} we use a topology with one region (no latencies) to verify some basic properties of crawl.js (e.g. scaling). In \emph{Experiment 2 \& 3} we introduce a new topology with 2 regions and focus on the \emph{closeness} aspect of crawl.js.
\\
Additionally we implemented a novel \emph{mapper} used throughout all our experiments.

\section{Measurement}
Throughout our experiments we use the unit pages/second to measure performance. We measure every worker independently, so the overall system performance is the sum of all measured worker performances. One worker measurement is done as follows:
\begin{itemize}
  \item We instrument how long it takes to crawl \emph{10'000 pages}
  \item Taking the arithmetic mean gives us \emph{pages/second} per worker
  \item Because we observed a big variance in our values we repeated one worker measurement several time and computed again the arithmetic mean.
\end{itemize}
Of course this unit depends highly on the worker hardware. Our numbers are base on a hardware configuration as detailed in Appendix~\ref{appendix:worker}

\section{2-level Mapper (Groups of workers)}
This novel mapper introduces the \emph{closeness} aspect. Closer sites (in terms of latency) should be crawled by workers near them. 
\newline
As the name suggests it, the URL to worker assignment is done using 2 levels. The first level (e.g. hostname) determines the group and the second level (e.g. path + query part) identifies the worker within that group. The 2-level mapper introduces the notion of groups to crawl.js and adds an additional dimension to the configuration. Previously we had 1 dimension: url-blocks. Now we have 2: groups \& blocks within that group. Note that each group can be configured independently. 1 group may have 1 worker and another 3 (e.g. depending on the number of URLs a group is responsible for).
\newline
With groups we have now the ability to configure crawl.js in such a way that \emph{closeness} is respected. For example we can create a group \emph{region 1} responsible for crawling sites \emph{close} to that region. How different group configurations affects the total crawl performance will be shown in experiment 3.

\section{Setup}
To do our experiments we had to setup the following types of VM in the cluster:
\begin{itemize}
\item www - static wikipedia snapshots.
\item redis - remote queues.
\item workers - crawls pages from \emph{www} and updates remote queues in \emph{redis}.
\end{itemize}
All VM's are based on a ubuntu server 12.04.3 LTS (64bit) image.

To setup and manage (workers) the VMs efficiently we wrote some basic bash scripts. It allowed us to deploy different worker configurations easily and perform basic operations on the workers such as start and stop. Having those scripts saved us a lot of time throughout the different experiments we ran. You find all scripts used throughout our experiments in the crawl.js git repository. In Figure~\ref{test_setup} is an overview of all the VMs used during the experiments.

\begin{figure}[h]
\centering
  \includegraphics[width=1.0\textwidth]{Figures/experiments_setup.pdf}
\caption{Crawl.js - Test setup overview}
\label{test_setup}
\end{figure}

\subsection{www - wikipedia snapshots}
In order to have realistic sites to crawl we decided to setup wikipedia html dumps. Unfortunately their dumper stopped back in 2008 and therefore the snapshots are pretty old. But for our use case it is good enough. Almost. Back then, the different wikipedia languages were located in different subdirectories. All on the same server. But we need links pointing to different servers to be able to add latency to some of the sites and experiment with the \emph{closeness} aspect of crawl.js. Therefore we wrote an xsl transformation script to change all language links from /de/index.html to de.wikipedia.org/index.html.


Additionally we encountered a serious performance problem during our first tests. The random read performance in our VM was about 200 IOPS (tested with fio). Therefore the web server (nginx) could not deliver pages fast enough and represented the bottleneck in our setup. In order to circumvent this unwanted side effect we put our html dumps on a ramfs.

\begin{itemize}
  \item CPU: 1, VCPU: 1
  \item RAM: 2048M
\end{itemize}

\subsection{redis - Remote queues}
This setup was pretty simple. Just a single redis server instance running. Because all the URLs have to fit in memory, setting up a redis cluster to share the key space on different servers is inevitable as soon the number of URLs becomes too big. In our simple/small setup this was not needed. During our experiments we encountered 217'690 URLs which represents 82M of data inside the redis server (redis info command).

\begin{itemize}
  \item CPU: 1, VCPU: 1
  \item RAM: 3072M
\end{itemize}

\subsection{worker - Crawl.js worker instance}
The worker VM hosts our developed crawler (crawl.js instance). Setting it up was straight forward. First we thought that we could host more than one crawler instance (using multiple cpus) but we achieved better results with one crawler per worker. This 1:1 worker to crawler relation made the management easier too.

\begin{itemize}
  \item CPU: 1, VCPU: 1
  \item RAM: 1024M
\end{itemize}

\section{Experiment 1}

The goal of our first experiment was to verify the following basic properties of crawl.js.

\begin{itemize}
  \item The crawl stops
  \item Adding more workers improves the overall crawl performance.
  \item All pages are found (starting with a single seed URL)
\end{itemize}

\subsection{Configuration}

Here is an overview of the configuration we used.

\begin{itemize}
  \item Workers: 1, 2, 3, 4 \& 5
  \item Sites: http://bn.wikipedia.org, http://bs.wikipedia.org
  \item Topology: 1 region (no latencies)
\end{itemize}

\subsection{Analysis}

All tests started with the same seed URL (http://bn.wikipedia.org). Verifying the first property \emph{the crawl stops} was straight forward and done manually. \emph{Adding more workers improves the overall crawl performance} was verified too as shown in Figure~\ref{plot:exp_001}. The scaling behaviour actually exceeded our expectations as it was linear. We were able to add workers and observe a constant increase of the whole system performance. Doing this we also observed that the performance per worker reaches its maximum only in a system with at least 3 workers (\~260 pages/second). We tried to explain this behaviour with bandwidth limitations a single worker has (100mbit), but our assumptions turned out to be wrong and we still can't explain it. Hopefully, a system with less than 3 workers is rather uncommon and we decided to not further investigate in finding the bottleneck causing this behaviour.
\newline
\newline
For the last property \emph{All pages are found} wikipedia provides a downloadable text file containing all article URLs. Using this file we were able to import the expected URLs into redis (set datastructure). Afterwards we took the difference (redis diff) between the two sets 'urls:expected' and 'urls:crawled'. Unfortunately the resulting set was not empty. It had 23'338 URLs. That number seemed us pretty huge, thats about 16\% of all articles. Did we miss that many URLs? We decided to have a closer look at some randomly picked articles to check what we actually missed. We discovered quickly that most of these 'missed' articles redirected (alias) to other articles we did crawl (13'391 redirect articles). Because articles never reference articles through an alias, crawl.js did not discover those 'redirect-articles' (as we follow references,<a> tags). This still leaves us with 9'947 (7\%) of potentially missed articles. Again we randomly picked some of the remaining articles and discovered that none of them were referenced by other articles (using grep -r). Of course we did not check for references to all remaining articles but 7\% of orphaned articles~\footnote{\url{http://en.wikipedia.org/wiki/Wikipedia:WikiProject_Orphanage}} seemed us realistic and therefore the verification of this property good enough to proceed with our experiments.
\newline
\newline
\begin{figure}
\centering
  \begin{tikzpicture} 
    \begin{axis}[
      ybar,
      bar width=12pt,
      enlargelimits=0.2,
      axis y line*=left,
      xlabel={Workers},
      xlabel near ticks,
      ylabel={Pages/second},
      ylabel near ticks,
      xtick=data,
      nodes near coords,
      nodes near coords align={vertical},
      ]
      \addplot
        coordinates {(1,169) (2,456) (3,850) (4,1069) (5,1326)};
    \end{axis}
  \end{tikzpicture}
\caption{Scaling behaviour (1, 2, 3, 4 \& 5 workers)}
\label{plot:exp_001}
\end{figure}

\section{Experiment 2}

In this experiment we took the same components (4 workers) as in experiment 1 but we introduced a new network topology with 2 regions as shown in Figure~\ref{test_setup2}. Basically we introduced two virtual regions by adding latencies between workers and wikipedia servers. The latency values used reflect the \emph{closeness} of the regions towards the wikipedia servers. With this new topology we want to demonstrate how drastically crawl performance suffers when dealing with latencies. Especially when using a random URL to worker assignment. In experiment 3 we will reconfigure the system in order to obtain better crawl performances with the same network topology in place.

\begin{figure}[h]
\centering
  \includegraphics[width=1.0\textwidth]{Figures/test_setup2.pdf}
  \caption{Network Topology: 2 regions (with latencies)}
\label{test_setup2}
\end{figure}

\subsection{Configuration}
\begin{itemize}
  \item Groups: 0: 4 blocks
  \item Workers: 4
  \item Sites: http://bs.wikipedia.org, http://bn.wikipedia.org
  \item Topologies: 1 region (no latencies) and 2 regions (Figure~\ref{test_setup2})
\end{itemize}

\subsection{Analysis}
Exactly as in experiment 1 all tests started with a single seed URL (http://bn.wikipedia.org). Figure~\ref{plot:exp_002} shows our measured performances, once for a network topology without latencies (from experiment 1) and once for the new network topology with latencies (2 regions). As you can see the performance difference is huge. Crawling on the second network topology is more than 9 times worse. Based on the latencies we added this observation can be explained as follows. Because we fetch many pages in parallel we first need to break down the performance to one connection:
\[ 
  P_c = P / c
\]
\begin{itemize}
  \item $ P1_c = 26.725 $ pages/second (P := 1069, c:= 40) 
  \item $ P2_c = 2.8 $ pages/second (P := 112, c:= 40)
\end{itemize}
Of course $ \frac{1}{P_c} $ is the time we need to crawl one page.
\\
\begin{itemize}
  \item $ \frac{1}{P1_c} = \frac{1}{26.725} $ seconds/page
  \item $ \frac{1}{P2_c} = \frac{1}{2.8} $ seconds/page
\end{itemize}
So, intuitively the difference $ P2_c - P1_c $ should roughly be equal to the latencies we added.
\[ \frac{1}{2.8} - \frac{1}{26.725} \approx  0.320 \]
But our expected mean latency is $ \approx 0.160 $ (based on the fraction pages on wiki-bs/wiki-bn). We miss a factor 2. Why?

\begin{figure}
  \centering
  \begin{tikzpicture} 
    \begin{axis}[
        ybar,
        bar width=12pt,
        axis y line*=left,
        enlargelimits=0.2,
        xlabel={Network topology},
        symbolic x coords={1 region, 2 regions},
        xlabel near ticks,
        ylabel={pages/second},
        ylabel near ticks,
        xtick=data,
        nodes near coords,
        nodes near coords align={vertical},
      ]
      \addplot
      coordinates {(1 region,1069) (2 regions,112)};
    \end{axis}
  \end{tikzpicture}
  \caption{Crawl performance on different network topologies}
  \label{plot:exp_002}
\end{figure}

\section{Experiment 3}

In the previous experiment we showed that latency on sites affects the crawl performance heavily. In this experiment we want to demonstrate that crawl.js is able to deal with different latencies on different sites. We want to show that when using the exact same latency configurations (100ms) as in experiment 2 we are able to achieve better performances by reconfiguring crawl.js. We will keep the number of workers the same but we will re-group them differently in order to respect \emph{closeness}.

\subsection{Configuration}
Test one exactly reflects the same setup as in experiment 2. 1 group with 4 workers and 100ms latency on bs.wikipedia.org. We repeat it just to confirm the measured results in experiment 2 and to prepare for the remaining tests. Because there is only one group and the URL to worker assignment is done randomly every worker is affected by the latency. In order to achieve a better performance we need workers that are not affected by the latency. Therefore we introduce a second group (Test 2 \& 3). One group is responsible for bs.wikipedia.org (100ms latency) and the other one for bn.wikipedia.org (no latency). With this setup we are be able to have workers that can operate on sites without (or with less) latency. We do 2 different tests with those 2 groups. First we keep the groups equally powerful by assigning 2 workers to each of them, second we rebalance the groups by moving 1 worker to the other group. This way we will have one group with a single worker responsible for bn.wikipedia.org (no latency) and another group with 3 workers responsible for bs.wikipedia.org (100ms latency). You can see the measured crawl durations in Figure~\ref{plot:exp_003}.
\begin{itemize}
  \item Mapper: 2-level
  \item Workers: 4
  \item Groups:
    \begin{itemize}
      \item Config 0: 1 group with 4 blocks
      \item Config 1: 2 groups with 2 blocks each
      \item Config 2: 1 group with 1 block, 1 group with 3 blocks
    \end{itemize}
  \item Sites: http://bs.wikipedia.org, http://bn.wikipedia.org
  \item Latency: 100ms on bs.wikipedia.org
\end{itemize}

\subsection{Analysis}

\begin{figure}
  \centering
  \begin{tikzpicture} 
    \begin{axis}[
        ybar,
        bar width=12pt,
        axis y line*=left,
        enlargelimits=0.2,
        xlabel={Group Configurations},
        xlabel near ticks,
        ylabel={Minutes},
        ylabel near ticks,
        xtick=data,
        nodes near coords,
        nodes near coords align={vertical},
      ]
      \addplot
      coordinates {(0,13.2) (1,25.05) (2,16.3)};
    \end{axis}
  \end{tikzpicture}
  \caption{Crawl performance using different group configurations}
  \label{plot:exp_003}
\end{figure}
